{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russia’s use of Twitter during the 2016 US Presidential Election\n",
    "---\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# import plotly.offline as py\n",
    "\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd() + \"/scripts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "df = pd.read_pickle('data/raw/tweets.pkl')\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only English language Tweets\n",
    "dfEng = pd.read_pickle('data/raw/tweetsEng.pkl')\n",
    "dfEng.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only non-English language Tweets\n",
    "dfOth = pd.read_pickle('data/raw/tweetsOth.pkl')\n",
    "dfOth.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polyglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so, 2): Symbol not found: __ZN6icu_648ByteSink15GetAppendBufferEiiPciPi\n  Referenced from: /Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so\n  Expected in: flat namespace\n in /Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-05968875281c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/polyglot/text.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextFiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_property\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/polyglot/detect/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLanguage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Detector'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Language'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/polyglot/detect/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0micu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpycld2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcld2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/icu/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0m_icu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so, 2): Symbol not found: __ZN6icu_648ByteSink15GetAppendBufferEiiPciPi\n  Referenced from: /Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so\n  Expected in: flat namespace\n in /Users/annakot/anaconda3/lib/python3.6/site-packages/_icu.cpython-36m-darwin.so"
     ]
    }
   ],
   "source": [
    "from polyglot.text import Text, Word"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>harvested_date</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>post_type</th>\n",
       "      <th>account_type</th>\n",
       "      <th>retweet</th>\n",
       "      <th>account_category</th>\n",
       "      <th>new_june_2018</th>\n",
       "      <th>alt_external_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>article_url</th>\n",
       "      <th>tco1_step1</th>\n",
       "      <th>tco2_step1</th>\n",
       "      <th>tco3_step1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2528776985</td>\n",
       "      <td>1488REASONS</td>\n",
       "      <td>Причина #67 Мутко: «Зенит-Арене» для адаптации...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1/19/2017 13:07</td>\n",
       "      <td>1/19/2017 13:07</td>\n",
       "      <td>6311</td>\n",
       "      <td>6313</td>\n",
       "      <td>1806</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>0</td>\n",
       "      <td>2528776985</td>\n",
       "      <td>822067907547066368</td>\n",
       "      <td>http://twitter.com/2528776985/statuses/8220679...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2528776985</td>\n",
       "      <td>1488REASONS</td>\n",
       "      <td>Причина #70 Житель Самары умер в очереди в пол...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1/22/2017 12:37</td>\n",
       "      <td>4/5/2017 7:17</td>\n",
       "      <td>1580</td>\n",
       "      <td>6298</td>\n",
       "      <td>1869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>0</td>\n",
       "      <td>2528776985</td>\n",
       "      <td>823147583895994368</td>\n",
       "      <td>http://twitter.com/2528776985/statuses/8231475...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2528776985</td>\n",
       "      <td>1488REASONS</td>\n",
       "      <td>Причина #74 Президентский советник предложил о...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1/26/2017 15:39</td>\n",
       "      <td>1/26/2017 15:40</td>\n",
       "      <td>6305</td>\n",
       "      <td>6312</td>\n",
       "      <td>1813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>0</td>\n",
       "      <td>2528776985</td>\n",
       "      <td>824643054351085568</td>\n",
       "      <td>http://twitter.com/2528776985/statuses/8246430...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2528776985</td>\n",
       "      <td>1488REASONS</td>\n",
       "      <td>Причина #75 Казаков благословили на защиту инт...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1/27/2017 13:23</td>\n",
       "      <td>1/27/2017 13:23</td>\n",
       "      <td>6304</td>\n",
       "      <td>6311</td>\n",
       "      <td>1814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>0</td>\n",
       "      <td>2528776985</td>\n",
       "      <td>824971050161209344</td>\n",
       "      <td>http://twitter.com/2528776985/statuses/8249710...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2528776985</td>\n",
       "      <td>1488REASONS</td>\n",
       "      <td>Причина #77 В Кстово полицейские сломали женщи...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1/29/2017 11:07</td>\n",
       "      <td>1/29/2017 11:30</td>\n",
       "      <td>6306</td>\n",
       "      <td>6308</td>\n",
       "      <td>1816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Russian</td>\n",
       "      <td>0</td>\n",
       "      <td>NonEnglish</td>\n",
       "      <td>0</td>\n",
       "      <td>2528776985</td>\n",
       "      <td>825661520726089728</td>\n",
       "      <td>http://twitter.com/2528776985/statuses/8256615...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  external_author_id       author  \\\n",
       "0         2528776985  1488REASONS   \n",
       "1         2528776985  1488REASONS   \n",
       "2         2528776985  1488REASONS   \n",
       "3         2528776985  1488REASONS   \n",
       "4         2528776985  1488REASONS   \n",
       "\n",
       "                                             content   region language  \\\n",
       "0  Причина #67 Мутко: «Зенит-Арене» для адаптации...  Unknown  Russian   \n",
       "1  Причина #70 Житель Самары умер в очереди в пол...  Unknown  Russian   \n",
       "2  Причина #74 Президентский советник предложил о...  Unknown  Russian   \n",
       "3  Причина #75 Казаков благословили на защиту инт...  Unknown  Russian   \n",
       "4  Причина #77 В Кстово полицейские сломали женщи...  Unknown  Russian   \n",
       "\n",
       "      publish_date   harvested_date  following  followers  updates post_type  \\\n",
       "0  1/19/2017 13:07  1/19/2017 13:07       6311       6313     1806       NaN   \n",
       "1  1/22/2017 12:37    4/5/2017 7:17       1580       6298     1869       NaN   \n",
       "2  1/26/2017 15:39  1/26/2017 15:40       6305       6312     1813       NaN   \n",
       "3  1/27/2017 13:23  1/27/2017 13:23       6304       6311     1814       NaN   \n",
       "4  1/29/2017 11:07  1/29/2017 11:30       6306       6308     1816       NaN   \n",
       "\n",
       "  account_type  retweet account_category  new_june_2018 alt_external_id  \\\n",
       "0      Russian        0       NonEnglish              0      2528776985   \n",
       "1      Russian        0       NonEnglish              0      2528776985   \n",
       "2      Russian        0       NonEnglish              0      2528776985   \n",
       "3      Russian        0       NonEnglish              0      2528776985   \n",
       "4      Russian        0       NonEnglish              0      2528776985   \n",
       "\n",
       "             tweet_id                                        article_url  \\\n",
       "0  822067907547066368  http://twitter.com/2528776985/statuses/8220679...   \n",
       "1  823147583895994368  http://twitter.com/2528776985/statuses/8231475...   \n",
       "2  824643054351085568  http://twitter.com/2528776985/statuses/8246430...   \n",
       "3  824971050161209344  http://twitter.com/2528776985/statuses/8249710...   \n",
       "4  825661520726089728  http://twitter.com/2528776985/statuses/8256615...   \n",
       "\n",
       "  tco1_step1 tco2_step1 tco3_step1  \n",
       "0        NaN        NaN        NaN  \n",
       "1        NaN        NaN        NaN  \n",
       "2        NaN        NaN        NaN  \n",
       "3        NaN        NaN        NaN  \n",
       "4        NaN        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfOth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
=======
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "# Let's get counts of the number of tweets by each author\n",
    "counts_by_author = dfEng[['author', 'content']].groupby('author').count()\n",
    "\n",
    "counts_by_author.reset_index(inplace = True)\n",
    "\n",
    "print(sum(counts_by_author.content > 400))\n",
    "\n",
    "author_series = counts_by_author.author[counts_by_author.content > 400]\n",
    "\n",
    "dfEng = dfEng[dfEng.author.isin(author_series)]"
   ]
  },
  {
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, natural language processing can be computationally expensive. Since we are dealing with a large number of tweets, it makes sense to take a sample of the data that we want to work with. \n",
    "\n",
    "Here, I take a 30% sample of tweets from each account. \n",
    "\n",
    "Note: the 30% sample size is arbitrary."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEng.content = dfEng.content.astype(str)\n",
    "\n",
    "# Create the groupby object of author and category. Note: every author is assigned one category\n",
    "df_sampled = dfEng.groupby(['author', 'account_category'])['content']\n",
    "\n",
    "# Take a 30% sample from each group\n",
    "df_sampled = df_sampled.apply(lambda x: x.sample(frac=0.3, replace = False)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be inclined to ask: why take a sample per account rather than just a sample of the overall dataset? When I first began expirimenting with this data, I tried to embed each individual tweet in a vector space. However, this procedure produced nonsensical results. It seems that there is not enough information contained in an individual tweet for us to gleen any useful unsights about the author, or discriminate between authors. \n",
    "\n",
    "However, if we were to combine tweets per author such that we have a single document for each author that is representative of everything that a particular author has ever tweeted, we will have more than enough information to give doc2vec a chance to work. \n",
    "\n",
    "There is just one problem: spaCy breaks if we try to parse long documents of text. As such, we will need to group our tweets by author after running the data through spaCy. \n",
    "\n",
    "Before we get to spaCy, we need to do some minor preprocessing. Here, I remove hyperlinks and and strip redundant whitepace from the text. I also expirimented with removing the RT symbol, but this did not seem to have any meaningful impact on our analysis. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE AND PRE-CLEANING\n",
    "\n",
    "# REMOVE RT SYMBOL\n",
    "#from processing_functions import rt_remover\n",
    "#df_sampled.content = df_sampled.content.apply(rt_remover)\n",
    "\n",
    "# REMOVE ANY HYPERLINKS\n",
    "from processing_functions import link_remover\n",
    "df_sampled.content = df_sampled.content.apply(link_remover)\n",
    "\n",
    "\n",
    "# STRIP ANY WHITESPACE ON EITHER SIDE OF THE TEXT\n",
    "#df_sampled.content = df_sampled.content.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some basic pre-processing, we can begin to parse and clean our text with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, cleaning, and lemmatization with spaCy\n",
    "\n",
    "- Note: this takes approximately 5 - 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 6,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# This allows us to add custom attributes to tokens, in this case, hashtags and accounts\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "# These functions tell spaCy what should be considered a hashtage or account\n",
    "from processing_functions import hashtag_pipe\n",
    "from processing_functions import is_account_pipe\n",
    "\n",
    "# We can  disable pipeline objects to save time: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en\", disable = ['parser', 'ner'])\n",
    "\n",
    "# Here I add the two custom functions for hashtags and accounts to the pipeline\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "nlp.add_pipe(is_account_pipe)\n",
    "\n",
    "# And we're off!\n",
    "parsed_tweets = list(nlp.pipe(df_sampled.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "\n",
    "Before we train our gensim doc2vec model, we need to do some more cleaning. The clean_doc function is located in the processing_functions.py file in this repository. \n",
    "\n",
    "Note: While I tried running the following without lemmatizing or removing stop words, I did not find that this made any meaningful difference in terms of the clusters that do or do not form."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 7,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import clean_doc\n",
    "from processing_functions import clean_doc_no_lemma\n",
    "\n",
    "lemma = list(map(clean_doc, parsed_tweets))\n",
    "#no_lemma = list(map(clean_doc_no_lemma, parsed_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is perhaps the most critical to this analysis. We need to group the parsed and cleaned tweets by author, so that for each author, we have a 30% sample of everything that they have ever tweeted in a single list (as discussed above). We can easily do this by applying our own function to a Pandas groupby object. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 8,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in parsed content is a list of lists\n",
    "df_sampled['parsed_content'] = lemma\n",
    "\n",
    "# lets flatten these so that each row only has one list\n",
    "from processing_functions import group_lists\n",
    "        \n",
    "df_grouped = df_sampled.groupby(['author', \n",
    "                    'account_category'])['parsed_content'].apply(group_lists).reset_index()\n",
    "\n",
    "# Convert the parsed content Series to a list for gensim\n",
    "parsed_content = list(df_grouped.parsed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that out data is clean and in the proper format, we can go ahead and create our gensim doc2vec model. It never ceases to amaze me that we can implement such a poweful algorithm in just a few lines of code. "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 9,
>>>>>>> c2cd270f13af649a4b96d5ef915da077a49ec054
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc2vec model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# parsed_content is the list of parsed text\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(parsed_content)]\n",
    "\n",
    "# Vector size of 300\n",
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=3, workers=6)\n",
    "\n",
    "arr_list = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    arr_list.append(model.docvecs[index])\n",
    "    \n",
    "vec_array = np.stack(arr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
