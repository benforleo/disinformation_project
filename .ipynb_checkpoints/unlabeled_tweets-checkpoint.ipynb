{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('ira_tweets_csv_hashed.csv', low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTER DOWN TO ENGLISH TWEETS AND A SUBSET OF COLUMNS\n",
    "# en_tweets = df[df.tweet_language == \"en\"]\n",
    "\n",
    "# col_list = ['tweetid', 'userid','tweet_text', 'tweet_time']\n",
    "\n",
    "# en_tweets = en_tweets.loc[:,col_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start here for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tweets = pd.read_csv(\"/Users/benjaminforleo/Box/spring_project/en_tweets.csv\", \n",
    "                        low_memory = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_tweets = en_tweets.drop_duplicates(subset = 'tweet_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE AND PRE-CLEANING\n",
    "tweet_sample = en_tweets.sample(50000)\n",
    "\n",
    "# REMOVE RT SYMBOL\n",
    "from processing_functions import rt_remover\n",
    "\n",
    "tweet_sample.tweet_text = tweet_sample.tweet_text.apply(rt_remover)\n",
    "\n",
    "tweet_sample.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "##### import functions, preprocess and tokenize the text\n",
    "\n",
    "from processing_functions import hashtag_pipe\n",
    "from processing_functions import is_account_pipe\n",
    "\n",
    "# to disable pipeline objects: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en\")\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "nlp.add_pipe(is_account_pipe)\n",
    "\n",
    "#en_tweets.tweet_text = en_tweets.tweet_text.apply(preprocessing)\n",
    "\n",
    "parsed_tweets = list(nlp.pipe(tweet_sample.tweet_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this applies the orginal cleaning function: it removes RT's stopwords and links. It currently returns the lowercase form of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from processing_functions import clean_doc\n",
    "\n",
    "# test = list(map(clean_doc, parsed_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import lower_case\n",
    "\n",
    "#list(map(lower_case, parsed_tweets))\n",
    "\n",
    "test = parsed_tweets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the gensim doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc2vec model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(test)]\n",
    "\n",
    "model = Doc2Vec(documents, vector_size=2, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_list = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    arr_list.append(model.docvecs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array= np.stack(arr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "clusters = list(range(1,11))\n",
    "\n",
    "for k in clusters:\n",
    "    \n",
    "    model = KMeans(n_clusters = k)\n",
    "    \n",
    "    model.fit(test_array)\n",
    "    \n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(clusters, inertias)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
