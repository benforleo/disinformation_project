{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russiaâ€™s use of Twitter during the 2016 US Presidential Election\n",
    "---\n",
    "\n",
    "Last updated by Benjamin Forleo 06/14/19\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd() + \"/scripts\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "df = pd.read_pickle('data/raw/tweets.pkl')\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only English language Tweets\n",
    "dfEng = pd.read_pickle('data/raw/tweetsEng.pkl')\n",
    "dfEng.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only non-English language Tweets\n",
    "dfOth = pd.read_pickle('data/raw/tweetsOth.pkl')\n",
    "dfOth.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: For the time being, I am using only English tweets to replicate what I did originally. - BF 06/01/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "# Let's get counts of the number of tweets by each author\n",
    "counts_by_author = dfEng[['author', 'content']].groupby('author').count()\n",
    "\n",
    "counts_by_author.reset_index(inplace = True)\n",
    "\n",
    "print(sum(counts_by_author.content > 400))\n",
    "\n",
    "author_series = counts_by_author.author[counts_by_author.content > 400]\n",
    "\n",
    "dfEng = dfEng[dfEng.author.isin(author_series)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, natural language processing can be computationally expensive. Since we are dealing with a large number of tweets, it makes sense to take a sample of the data that we want to work with. \n",
    "\n",
    "Here, we take a 30% sample of tweets from each account. Later, we will group the tweets by author such that each line in our data frame contains one author and a 30% sample of everything that author has ever tweeted.\n",
    "\n",
    "Note: the 30% sample size is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEng.content = dfEng.content.astype(str)\n",
    "\n",
    "# Create the groupby object of author and category. Note: every author is assigned one category\n",
    "df_sampled = dfEng.groupby(['author', 'account_category'])['content']\n",
    "\n",
    "# Take a 30% sample from each group\n",
    "df_sampled = df_sampled.apply(lambda x: x.sample(frac=0.3, replace = False)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE AND PRE-CLEANING\n",
    "\n",
    "# REMOVE RT SYMBOL\n",
    "#from processing_functions import rt_remover\n",
    "#df_sampled.content = df_sampled.content.apply(rt_remover)\n",
    "\n",
    "# REMOVE ANY HYPERLINKS\n",
    "from processing_functions import link_remover\n",
    "df_sampled.content = df_sampled.content.apply(link_remover)\n",
    "\n",
    "\n",
    "# STRIP ANY WHITESPACE ON EITHER SIDE OF THE TEXT\n",
    "df_sampled.content = df_sampled.content.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some basic pre-processing, we can begin to parse and clean our text with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, cleaning, and lemmatization with spaCy\n",
    "\n",
    "- Note: this takes approximately 5 - 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# This allows us to add custom attributes to tokens, in this case, hashtags and accounts\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "# These functions tell spaCy what should be considered a hashtage or account\n",
    "from processing_functions import hashtag_pipe\n",
    "from processing_functions import is_account_pipe\n",
    "\n",
    "# We can  disable pipeline objects to save time: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en\", disable = ['parser', 'ner'])\n",
    "\n",
    "# Here I add the two custom functions for hashtags and accounts to the pipeline\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "nlp.add_pipe(is_account_pipe)\n",
    "\n",
    "# And we're off!\n",
    "parsed_tweets = list(nlp.pipe(df_sampled.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "\n",
    "Before we train our gensim doc2vec model, we need to do some more cleaning. The clean_doc function is located in the processing_functions.py file in this repository. \n",
    "\n",
    "Note: While we tried running the following without lemmatizing or removing stop words, we did not find that this made any meaningful difference in terms of the clusters that do or do not form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import clean_doc\n",
    "from processing_functions import clean_doc_no_lemma\n",
    "\n",
    "lemma = list(map(clean_doc, parsed_tweets))\n",
    "#no_lemma = list(map(clean_doc_no_lemma, parsed_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is perhaps the most critical to this analysis. We need to group the parsed and cleaned tweets by author, so that for each author, we have a 30% sample of everything that they have ever tweeted in a single list (as discussed above). We can easily do this by applying our own function to a Pandas groupby object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in parsed content is a list of lists\n",
    "df_sampled['parsed_content'] = lemma\n",
    "\n",
    "# lets flatten these so that each row only has one list\n",
    "from processing_functions import group_lists\n",
    "        \n",
    "df_grouped = df_sampled.groupby(['author', \n",
    "                    'account_category'])['parsed_content'].apply(group_lists).reset_index()\n",
    "\n",
    "# Convert the parsed content Series to a list for gensim\n",
    "parsed_content = list(df_grouped.parsed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that out data is clean and in the proper format, we can go ahead and create our gensim doc2vec model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc2vec model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# parsed_content is the list of parsed text\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(parsed_content)]\n",
    "\n",
    "# Vector size of 300\n",
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=3, workers = 6)\n",
    "\n",
    "# save the model to disk\n",
    "model.save('./saved_models/eng_doc2vec/eng_gensim_model')\n",
    "\n",
    "arr_list = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    arr_list.append(model.docvecs[index])\n",
    "    \n",
    "vec_array = np.stack(arr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge docVec labels and save as csv\n",
    "vec_df = pd.DataFrame(vec_array)\n",
    "\n",
    "vec_df.insert(0, 'account_category', df_grouped.account_category)\n",
    "vec_df.insert(0, 'author', df_grouped.author)\n",
    "\n",
    "vec_df.to_csv('./data/eng_labeled_docvecs.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
