{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russiaâ€™s use of Twitter during the 2016 US Presidential Election\n",
    "---\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "df = pd.read_pickle('data/raw/tweets.pkl')\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only English language Tweets\n",
    "dfEng = pd.read_pickle('data/raw/tweetsEng.pkl')\n",
    "dfEng.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only non-English language Tweets\n",
    "dfOth = pd.read_pickle('data/raw/tweetsOth.pkl')\n",
    "dfOth.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, natural language processing can be computationally expensive. Since we are dealing with a large number of tweets, it makes sense to take a sample of the data that we want to work with. \n",
    "\n",
    "Here, I take a 30% sample of tweets from each account. \n",
    "\n",
    "Note: the 30% sample size is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content = df.content.astype(str)\n",
    "\n",
    "# Create the groupby object of author and category. Note: every author is assigned one category\n",
    "df_sampled = df.groupby(['author', 'account_category'])['content']\n",
    "\n",
    "# Take a 30% sample from each group\n",
    "df_sampled = df_sampled.apply(lambda x: x.sample(frac=0.3, replace = False)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be inclined to ask: why take a sample per account rather than just a sample of the overall dataset? When I first began expirimenting with this data, I tried to embed each individual tweet in a vector space. However, this procedure produced nonsensical results. It seems that there is not enough information contained in an individual tweet for us to gleen any useful unsights about the author, or discriminate between authors. \n",
    "\n",
    "However, if we were to combine tweets per author such that we have a single document for each author that is representative of everything that a particular author has ever tweeted, we will have more than enough information to give doc2vec a chance to work. \n",
    "\n",
    "There is just one problem: spaCy breaks if we try to parse long documents of text. As such, we will need to group our tweets by author after running the data through spaCy. \n",
    "\n",
    "Before we get to spaCy, we need to do some minor preprocessing. Here, I remove hyperlinks and and strip redundant whitepace from the text. I also expirimented with removing the RT symbol, but this did not seem to have any meaningful impact on our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE AND PRE-CLEANING\n",
    "\n",
    "# REMOVE RT SYMBOL\n",
    "#from processing_functions import rt_remover\n",
    "#df_sampled.content = df_sampled.content.apply(rt_remover)\n",
    "\n",
    "# REMOVE ANY HYPERLINKS\n",
    "from processing_functions import link_remover\n",
    "df_sampled.content = df_sampled.content.apply(link_remover)\n",
    "\n",
    "\n",
    "# STRIP ANY WHITESPACE ON EITHER SIDE OF THE TEXT\n",
    "df_sampled.content = df_sampled.content.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some basic pre-processing, we can begin to parse and clean our text with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, cleaning, and lemmatization with spaCy\n",
    "\n",
    "- Note: this takes approximately 5 - 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# This allows us to add custom attributes to tokens, in this case, hashtags and accounts\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "# These functions tell spaCy what should be considered a hashtage or account\n",
    "from processing_functions import hashtag_pipe\n",
    "from processing_functions import is_account_pipe\n",
    "\n",
    "# We can  disable pipeline objects to save time: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en\", disable = ['parser', 'ner'])\n",
    "\n",
    "# Here I add the two custom functions for hashtags and accounts to the pipeline\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "nlp.add_pipe(is_account_pipe)\n",
    "\n",
    "# And we're off!\n",
    "parsed_tweets = list(nlp.pipe(df_sampled.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "\n",
    "Before we train our gensim doc2vec model, we need to do some more cleaning. The clean_doc function is located in the processing_functions.py file in this repository. \n",
    "\n",
    "Note: While I tried running the following without lemmatizing or removing stop words, I did not find that this made any meaningful difference in terms of the clusters that do or do not form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import clean_doc\n",
    "from processing_functions import clean_doc_no_lemma\n",
    "\n",
    "lemma = list(map(clean_doc, parsed_tweets))\n",
    "#no_lemma = list(map(clean_doc_no_lemma, parsed_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is perhaps the most critical to this analysis. We need to group the parsed and cleaned tweets by author, so that for each author, we have a 30% sample of everything that they have ever tweeted in a single list (as discussed above). We can easily do this by applying our own function to a Pandas groupby object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in parsed content is a list of lists\n",
    "df_sampled['parsed_content'] = lemma\n",
    "\n",
    "# lets flatten these so that each row only has one list\n",
    "from processing_functions import group_lists\n",
    "        \n",
    "df_grouped = df_sampled.groupby(['author', \n",
    "                    'account_category'])['parsed_content'].apply(group_lists).reset_index()\n",
    "\n",
    "# Convert the parsed content Series to a list for gensim\n",
    "parsed_content = list(df_grouped.parsed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that out data is clean and in the proper format, we can go ahead and create our gensim doc2vec model. It never ceases to amaze me that we can implement such a poweful algorithm in just a few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc2vec model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# parsed_content is the list of parsed text\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(parsed_content)]\n",
    "\n",
    "# Vector size of 300\n",
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=3, workers=6)\n",
    "\n",
    "arr_list = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    arr_list.append(model.docvecs[index])\n",
    "    \n",
    "vec_array = np.stack(arr_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
