{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russiaâ€™s use of Twitter during the 2016 US Presidential Election\n",
    "---\n",
    "\n",
    "Last updated by Benjamin Forleo 06/01/19\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "# from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "# import plotly.offline as py\n",
    "\n",
    "from plotly.offline import init_notebook_mode, plot, iplot\n",
    "import plotly.io as pio\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'RightTroll': 223, 'LeftTroll': 118, 'HashtagGamer': 63, 'NewsFeed': 45, 'NonEnglish': 7, 'Commercial': 5, 'Unknown': 2})\n"
     ]
    }
   ],
   "source": [
    "# English Language Tweets (Clemson Dataset)\n",
    "vec_df = pd.read_csv('./data/eng_labeled_docvecs.csv')\n",
    "\n",
    "vec_array = np.array(vec_df.iloc[:,2:])\n",
    "\n",
    "x = np.array(vec_df.iloc[:,2:])\n",
    "y = vec_df.account_category.copy()\n",
    "\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (Work in Progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to check our class balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the Limited number of samples, I am going to classify NonEnglish, Commercial, and Unknown as Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_list = ['NonEnglish','Commercial', 'Unknown']\n",
    "\n",
    "y = y.apply(lambda x: 'Other' if x in other_list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y)\n",
    "\n",
    "sm = SMOTE(random_state = 123, k_neighbors = 6)\n",
    "\n",
    "x_res, y_res = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg = OneVsRestClassifier(LogisticRegression(solver = 'liblinear'))\n",
    "\n",
    "logreg.fit(x_res, y_res)\n",
    "\n",
    "# Save the logreg model\n",
    "filename = './saved_models/eng_predictive_models/eng_logreg.sav'\n",
    "joblib.dump(logreg, filename)\n",
    "\n",
    "y_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9913793103448276 \n",
      " \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "HashtagGamer       1.00      1.00      1.00        16\n",
      "   LeftTroll       1.00      0.97      0.98        30\n",
      "    NewsFeed       1.00      1.00      1.00        11\n",
      "       Other       1.00      1.00      1.00         3\n",
      "  RightTroll       0.98      1.00      0.99        56\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       116\n",
      "   macro avg       1.00      0.99      0.99       116\n",
      "weighted avg       0.99      0.99      0.99       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Score: \",accuracy_score(y_test, y_pred), \"\\n\", \"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score:  0.9827586206896551 \n",
      " \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "HashtagGamer       1.00      1.00      1.00        16\n",
      "   LeftTroll       1.00      0.93      0.97        30\n",
      "    NewsFeed       1.00      1.00      1.00        11\n",
      "       Other       1.00      1.00      1.00         3\n",
      "  RightTroll       0.97      1.00      0.98        56\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       116\n",
      "   macro avg       0.99      0.99      0.99       116\n",
      "weighted avg       0.98      0.98      0.98       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = OneVsRestClassifier(SVC(gamma = 'auto'))\n",
    "\n",
    "param_grid = {'estimator__C': [1, 3, 5, 7]}\n",
    "\n",
    "grid_svc = GridSearchCV(estimator = svc, param_grid = param_grid, \n",
    "                        scoring = 'accuracy', n_jobs = -1, verbose = 0,\n",
    "                       cv = 4)\n",
    "\n",
    "grid_svc.fit(x_res, y_res)\n",
    "\n",
    "# Save the SVM model\n",
    "filename = './saved_models/eng_predictive_models/eng_svm.sav'\n",
    "joblib.dump(grid_svc.best_estimator_, filename)\n",
    "\n",
    "y_pred_svc = grid_svc.predict(x_test)\n",
    "\n",
    "print('SVM Accuracy Score: ',accuracy_score(y_test, y_pred_svc), \"\\n\", \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9655172413793104 \n",
      " \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "HashtagGamer       1.00      1.00      1.00        16\n",
      "   LeftTroll       1.00      0.90      0.95        30\n",
      "    NewsFeed       1.00      1.00      1.00        11\n",
      "       Other       0.50      1.00      0.67         3\n",
      "  RightTroll       0.98      0.98      0.98        56\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       116\n",
      "   macro avg       0.90      0.98      0.92       116\n",
      "weighted avg       0.98      0.97      0.97       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 123, n_estimators = 10)\n",
    "\n",
    "\n",
    "params_rf = {'max_depth':[2,3,4,5,6],\n",
    "             'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "             'max_features': [0.2, 0.4, 0.6, 0.8],\n",
    "             'criterion':['gini', 'entropy']}\n",
    "             \n",
    "grid_rf = GridSearchCV(estimator = rf, param_grid = params_rf, \n",
    "                       scoring = 'accuracy', cv= 10, n_jobs = -1, iid = True)\n",
    "\n",
    "\n",
    "grid_rf.fit(x_res, y_res)\n",
    "\n",
    "filename = './saved_models/eng_predictive_models/eng_rf.sav'\n",
    "joblib.dump(grid_rf.best_estimator_, filename)\n",
    "\n",
    "y_pred_rf = grid_rf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred_rf), \"\\n\", \"\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminforleo/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning:\n",
      "\n",
      "The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score Train CV:  0.9880239520958084\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid = {\n",
    "  'learning_rate': np.arange(0.05, 1.05, .10),\n",
    "  'n_estimators' : [50],\n",
    "  'subsample' : np.arange(0.05, 1.05, .05),\n",
    "  'max_depth': [2,4,6]\n",
    "  }\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective = 'multi:softmax')\n",
    "\n",
    "randomized_xg_cl = RandomizedSearchCV(estimator = xg_cl, \n",
    "                                      param_distributions = param_grid,\n",
    "                                      n_iter = 5, \n",
    "                                      cv = 5,\n",
    "                                      scoring = 'accuracy',\n",
    "                                      n_jobs = -1,\n",
    "                                      verbose = 0)\n",
    "\n",
    "randomized_xg_cl.fit(x_res, y_res)\n",
    "\n",
    "filename = './saved_models/eng_predictive_models/eng_xgb.sav'\n",
    "joblib.dump(randomized_xg_cl.best_estimator_, filename)\n",
    "\n",
    "print(\"Best Accuracy Score Train CV: \", randomized_xg_cl.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Test Accuracy:  0.9655172413793104 \n",
      " \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "HashtagGamer       0.94      1.00      0.97        16\n",
      "   LeftTroll       1.00      0.93      0.97        30\n",
      "    NewsFeed       1.00      1.00      1.00        11\n",
      "       Other       1.00      0.67      0.80         3\n",
      "  RightTroll       0.95      0.98      0.96        56\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       116\n",
      "   macro avg       0.98      0.92      0.94       116\n",
      "weighted avg       0.97      0.97      0.96       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = randomized_xg_cl.predict(x_test)\n",
    "\n",
    "print('XGBoost Test Accuracy: ', accuracy_score(y_test, y_pred), \"\\n\", \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
