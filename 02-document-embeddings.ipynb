{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrapped-internet",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russiaâ€™s use of Twitter during the 2016 US Presidential Election\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "built-holiday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from processing_modules.io_data import ImportData\n",
    "from processing_modules.text_processing import link_remover, hashtag_token, account_token, group_lists,\\\n",
    "                                                lemmatize_and_clean_document, clean_document, retweet_remover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-smell",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "featured-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "portfolio_data_bucket = os.getenv(\"PORTFOLIO_DATA_BUCKET\")\n",
    "\n",
    "five_38_data_retrieval = ImportData(\n",
    "    bucket=portfolio_data_bucket,\n",
    "    prefix=\"disinformation-project/raw-data\"\n",
    ")\n",
    "\n",
    "df = five_38_data_retrieval\\\n",
    "        .retrieve_objects()\\\n",
    "        .return_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-albany",
   "metadata": {},
   "source": [
    "## Sample and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "missing-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_categories = ['RightTroll','LeftTroll', 'HashtagGamer', 'NewsFeed']\n",
    "df = df[(df.language == 'English') & (df.account_category.isin(account_categories))]\n",
    "\n",
    "# need to remove accounts that tweet less than 400 times. \n",
    "author_counts = df.groupby('author')['tweet_id'].count()\n",
    "relevant_authors = author_counts[author_counts > 200].index.values.tolist()\n",
    "df = df[df.author.isin(relevant_authors)]\n",
    "\n",
    "sample = df.groupby(['author', 'account_category'])[['content']]\\\n",
    "            .apply(lambda x: x.sample(frac=.3, replace=False))\\\n",
    "            .reset_index()\\\n",
    "            .drop(\"level_2\", axis='columns')\n",
    "\n",
    "sample.content = sample.content\\\n",
    "                    .astype(str).str.strip()\\\n",
    "                    .apply(link_remover)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-subsection",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "strong-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to add custom attributes to tokens, in this case, hashtags and accounts\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "# We can  disable pipeline objects to save time: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner', 'textcat'])\n",
    "\n",
    "# Here I add the two custom functions for hashtags and accounts to the pipeline\n",
    "nlp.add_pipe(hashtag_token)\n",
    "nlp.add_pipe(account_token)\n",
    "\n",
    "# And we're off!\n",
    "parsed_tweets = list(nlp.pipe(sample.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-volleyball",
   "metadata": {},
   "source": [
    "#### Additional text cleaning and aggregation\n",
    "\n",
    "Here we:\n",
    "- further clean tweets by filter tokens (remove punctuation, spaces, etc.) and\n",
    "- group tweet documents by author "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "certain-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['processed_tweets'] = list(map(lemmatize_and_clean_document, parsed_tweets))\n",
    "#sample['processed_tweets']= list(map(clean_document, parsed_tweets))\n",
    "\n",
    "grouped_tweets = sample.groupby(['author', 'account_category'])['processed_tweets']\\\n",
    "                    .apply(group_lists)\\\n",
    "                    .reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-blank",
   "metadata": {},
   "source": [
    "#### Document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "integrated-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_content is the list of parsed text\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(grouped_tweets.processed_tweets)]\n",
    "\n",
    "# Vector size of 300\n",
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=3, workers = 6)\n",
    "\n",
    "# save the model to disk\n",
    "model.save('./models/doc2vec/disinformation-project-doc2vec')\n",
    "\n",
    "document_vectors = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    document_vectors.append(model.docvecs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "instrumental-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dataframe = pd.DataFrame(np.stack(document_vectors))\n",
    "\n",
    "embedding_dataframe.insert(0, 'account_category', grouped_tweets.account_category)\n",
    "embedding_dataframe.insert(0, 'author', grouped_tweets.author)\n",
    "\n",
    "embedding_dataframe.to_csv(\"./data/tweet-embeddings.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
