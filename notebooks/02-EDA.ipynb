{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Warfare\n",
    "## Russiaâ€™s use of Twitter during the 2016 US Presidential Election\n",
    "---\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', None)\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\"\n",
    "\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.offline as py\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Tweets\n",
    "df = pd.read_pickle('data/raw/tweets.pkl')\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only English language Tweets\n",
    "dfEng = pd.read_pickle('data/raw/tweetsEng.pkl')\n",
    "dfEng.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Only non-English language Tweets\n",
    "dfOth = pd.read_pickle('data/raw/tweetsOth.pkl')\n",
    "dfOth.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Tweets: \n",
      " 2,034,154 rows of data, with 21 observations for each row \n",
      " 463 unique accounts \n",
      " \n",
      " Account types: ['Hashtager', '?', 'Left', 'Right', 'Arabic', 'Italian', 'news', 'Russian', 'Commercial', 'local'] \n",
      " Account categories: ['Commercial', 'RightTroll', 'HashtagGamer', 'NewsFeed', 'Unknown', 'LeftTroll', 'NonEnglish'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All Tweets\n",
    "print(\"All Tweets: \\n {:,} rows of data, with {} observations for each row \\n {:,} unique accounts \\n \\n Account types: {} \\n Account categories: {} \\n\".format(df.shape[0], df.shape[1],len(set(df.author)),list(set(df.account_type)),list(set(df.account_category))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tweets: \n",
      " 2,116,867 rows of data, with 21 observations for each row \n",
      " 2,161 unique accounts \n",
      " \n",
      " Account types: ['Hashtager', '?', 'French', 'Left', 'Right', 'Arabic', 'Italian', 'news', 'Russian', 'Commercial', 'ZAPOROSHIA', 'Spanish', 'German', 'Ebola ', 'Koch', 'Portuguese', 'local'] \n",
      " \n",
      " Account categories: ['Commercial', 'Fearmonger', 'RightTroll', 'HashtagGamer', 'NewsFeed', 'Unknown', 'LeftTroll', 'NonEnglish'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only English language Tweets\n",
    "print(\"English Tweets: \\n {:,} rows of data, with {} observations for each row \\n {:,} unique accounts \\n \\n Account types: {} \\n \\n Account categories: {} \\n\".format(dfEng.shape[0], dfEng.shape[1],len(set(dfEng.author)),list(set(dfEng.account_type)),list(set(dfEng.account_category))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-English Tweets: \n",
      " 829,340 rows of data, with 21 observations for each row \n",
      " 2,360 unique accounts \n",
      " \n",
      " Account types: ['Ukranian', 'Hashtager', '?', 'French', 'Left', 'Right', 'Arabic', 'Italian', 'local', 'news', 'Russian', 'Commercial', 'ZAPOROSHIA', 'Uzbek', 'German', 'Ebola ', 'Koch', 'Portuguese', 'Spanish'] \n",
      " \n",
      " Account categories: ['Commercial', 'Fearmonger', 'RightTroll', 'HashtagGamer', 'NewsFeed', 'Unknown', 'LeftTroll', 'NonEnglish'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only non-English language Tweets\n",
    "print(\"Non-English Tweets: \\n {:,} rows of data, with {} observations for each row \\n {:,} unique accounts \\n \\n Account types: {} \\n \\n Account categories: {} \\n\".format(dfOth.shape[0], dfOth.shape[1],len(set(dfOth.author)),list(set(dfOth.account_type)),list(set(dfOth.account_category))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two sets of labels that Linville and Warren have created, namely, account type and account name. Account type consists of more granular labels, while account category is more broad. For this exercise, I will focus on the account category set of labels. \n",
    "\n",
    "Before we move on, I should point out that the labeling scheme employed by Linville and Warren allows for accounts that primarily tweet in English to be labeled as Non English. For example, an account that primarily tweets about international events, such as the war in Ukraine, would be labeled as Non English, even if the text is in English. The graph below indicates that activity characterized by this behavior is limited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfEng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(175, 88, 141)",
           "rgb(175, 88, 141)",
           "rgb(175, 88, 141)",
           "rgb(175, 88, 141)"
          ]
         },
         "opacity": 0.7,
         "type": "bar",
         "uid": "829ce4b1-6d03-47f8-b3d6-c3ac7955a629",
         "x": [
          "RightTroll",
          "NewsFeed",
          "LeftTroll",
          "HashtagGamer",
          "Commercial",
          "NonEnglish",
          "Fearmonger",
          "Unknown"
         ],
         "y": [
          704953,
          596593,
          422141,
          236092,
          112580,
          26562,
          11001,
          6945
         ]
        }
       ],
       "layout": {
        "title": {
         "font": {
          "size": 30
         },
         "text": "Counts of Tweets by Category"
        },
        "xaxis": {
         "automargin": true,
         "tickangle": 45,
         "tickfont": {
          "size": 15
         }
        },
        "yaxis": {
         "title": {
          "font": {
           "size": 20
          },
          "text": "Tweets"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"5d5a6a46-da63-44fa-98e8-7dde45159aa5\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"5d5a6a46-da63-44fa-98e8-7dde45159aa5\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '5d5a6a46-da63-44fa-98e8-7dde45159aa5',\n",
       "                        [{\"marker\": {\"color\": [\"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(175, 88, 141)\", \"rgb(175, 88, 141)\", \"rgb(175, 88, 141)\", \"rgb(175, 88, 141)\"]}, \"opacity\": 0.7, \"type\": \"bar\", \"uid\": \"449145c6-a45a-4814-abde-59e54ec797b8\", \"x\": [\"RightTroll\", \"NewsFeed\", \"LeftTroll\", \"HashtagGamer\", \"Commercial\", \"NonEnglish\", \"Fearmonger\", \"Unknown\"], \"y\": [704953, 596593, 422141, 236092, 112580, 26562, 11001, 6945]}],\n",
       "                        {\"title\": {\"font\": {\"size\": 30}, \"text\": \"Counts of Tweets by Category\"}, \"xaxis\": {\"automargin\": true, \"tickangle\": 45, \"tickfont\": {\"size\": 15}}, \"yaxis\": {\"title\": {\"font\": {\"size\": 20}, \"text\": \"Tweets\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('5d5a6a46-da63-44fa-98e8-7dde45159aa5');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts_by_type = dict(Counter(df.account_category))\n",
    "\n",
    "account_type = list(counts_by_type.keys())\n",
    "values = list(counts_by_type.values())\n",
    "\n",
    "account_type = [x for _,x in sorted(zip(values ,account_type), reverse = True)]\n",
    "values = sorted(values, reverse = True)\n",
    "\n",
    "other_color = ['Unknown', 'Commercial', 'NonEnglish', 'Fearmonger']\n",
    "\n",
    "color_list = ['rgb(175, 88, 141)' if i in other_color else 'rgb(23,62,90)' for i in account_type]\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=account_type,\n",
    "            y=values,\n",
    "            marker = dict(color = color_list),\n",
    "            opacity = .7\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(title = dict(text = 'Counts of Tweets by Category', font = dict(size = 30)), \n",
    "                   xaxis = dict(\n",
    "                       tickangle = 45, \n",
    "                       tickfont = dict(size = 15),\n",
    "                       automargin = True),\n",
    "                  yaxis = dict(title = dict(text = 'Tweets', font = dict(size = 20))))\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous chart displayed information for overall tweets. However, what we are really interested in is activity per account. That is, it would be nice to know how many right trolls are in the dataset vs. left trolls and so on. Let's go ahead and visualize this information now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           "rgb(175, 88, 141)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(175, 88, 141)",
           "rgb(175, 88, 141)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(175, 88, 141)"
          ]
         },
         "opacity": 0.7,
         "type": "bar",
         "uid": "c9b04aa6-cf81-42ba-9138-df98af281ac1",
         "x": [
          "NonEnglish",
          "RightTroll",
          "LeftTroll",
          "Unknown",
          "Fearmonger",
          "HashtagGamer",
          "NewsFeed",
          "Commercial"
         ],
         "y": [
          795,
          630,
          233,
          207,
          124,
          112,
          54,
          6
         ]
        }
       ],
       "layout": {
        "title": {
         "font": {
          "size": 30
         },
         "text": "Counts of Accounts by Category"
        },
        "xaxis": {
         "automargin": true,
         "tickangle": 45,
         "tickfont": {
          "size": 15
         }
        },
        "yaxis": {
         "title": {
          "font": {
           "size": 20
          },
          "text": "Accounts"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"330f8457-34f1-48bf-b9dc-a2ce323b6566\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"330f8457-34f1-48bf-b9dc-a2ce323b6566\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '330f8457-34f1-48bf-b9dc-a2ce323b6566',\n",
       "                        [{\"marker\": {\"color\": [\"rgb(175, 88, 141)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(175, 88, 141)\", \"rgb(175, 88, 141)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(175, 88, 141)\"]}, \"opacity\": 0.7, \"type\": \"bar\", \"uid\": \"e45851c7-d7c5-482a-b8f0-6797229e0325\", \"x\": [\"NonEnglish\", \"RightTroll\", \"LeftTroll\", \"Unknown\", \"Fearmonger\", \"HashtagGamer\", \"NewsFeed\", \"Commercial\"], \"y\": [795, 630, 233, 207, 124, 112, 54, 6]}],\n",
       "                        {\"title\": {\"font\": {\"size\": 30}, \"text\": \"Counts of Accounts by Category\"}, \"xaxis\": {\"automargin\": true, \"tickangle\": 45, \"tickfont\": {\"size\": 15}}, \"yaxis\": {\"title\": {\"font\": {\"size\": 20}, \"text\": \"Accounts\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('330f8457-34f1-48bf-b9dc-a2ce323b6566');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "account_analysis = df.groupby(['author', 'account_category'])['content'].count().reset_index()\n",
    "\n",
    "account_analysis = Counter(account_analysis.account_category)\n",
    "\n",
    "account_type = list(account_analysis.keys())\n",
    "values = list(account_analysis.values())\n",
    "\n",
    "account_type = [x for _,x in sorted(zip(values ,account_type), reverse = True)]\n",
    "values = sorted(values, reverse = True)\n",
    "\n",
    "color_list = ['rgb(175, 88, 141)' if i in other_color else 'rgb(23,62,90)' for i in account_type]\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=account_type,\n",
    "            y=values,\n",
    "            marker = dict(color = color_list),\n",
    "            opacity = .7\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(title = dict(text = 'Counts of Accounts by Category', font = dict(size = 30)), \n",
    "                   xaxis = dict(\n",
    "                       tickangle = 45, \n",
    "                       tickfont = dict(size = 15),\n",
    "                       automargin = True),\n",
    "                  yaxis = dict(title = dict(text = 'Accounts', font = dict(size = 20))))\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is an interesting chart. While Non English tweets account for very little of the overall tweet volume, there is a sizable number of Non English accounts in the dataset. \n",
    "\n",
    "For our procedure to work, we need to make sure that we have enough information (tweets) for each account for doc2vec to appropriatly embed our text in a vector space. As such, we need to ensure that we only consider accounts that meet some threshold for a minimum number of tweets, which we we do below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "# Let's get counts of the number of tweets by each author\n",
    "counts_by_author = df[['author', 'content']].groupby('author').count()\n",
    "\n",
    "counts_by_author.reset_index(inplace = True)\n",
    "\n",
    "print(sum(counts_by_author.content > 400))\n",
    "\n",
    "author_series = counts_by_author.author[counts_by_author.content > 400]\n",
    "\n",
    "df = df[df.author.isin(author_series)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the choice of 400 tweets is arbitrary. Notice that by setting this threshold, the amount of Non English accounts in the dataset has decreased considerably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "responsive": true,
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": [
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(23,62,90)",
           "rgb(175, 88, 141)",
           "rgb(23,62,90)",
           "rgb(23,62,90)"
          ]
         },
         "opacity": 0.7,
         "type": "bar",
         "uid": "e11e073d-e267-4734-9f7e-c6cf10fd033d",
         "x": [
          "RightTroll",
          "LeftTroll",
          "HashtagGamer",
          "NewsFeed",
          "NonEnglish",
          "Commercial",
          "Unknown"
         ],
         "y": [
          223,
          118,
          63,
          45,
          7,
          5,
          2
         ]
        }
       ],
       "layout": {
        "title": {
         "font": {
          "size": 30
         },
         "text": "Accounts with more than 400 Tweets"
        },
        "xaxis": {
         "automargin": true,
         "tickangle": 45,
         "tickfont": {
          "size": 15
         }
        },
        "yaxis": {
         "title": {
          "font": {
           "size": 20
          },
          "text": "Accounts"
         }
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"26500950-cd08-4c79-950c-0cff87461b28\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"26500950-cd08-4c79-950c-0cff87461b28\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '26500950-cd08-4c79-950c-0cff87461b28',\n",
       "                        [{\"marker\": {\"color\": [\"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\", \"rgb(175, 88, 141)\", \"rgb(23,62,90)\", \"rgb(23,62,90)\"]}, \"opacity\": 0.7, \"type\": \"bar\", \"uid\": \"696afb76-1748-45e0-bbd5-280f34c12942\", \"x\": [\"RightTroll\", \"LeftTroll\", \"HashtagGamer\", \"NewsFeed\", \"NonEnglish\", \"Commercial\", \"Unknown\"], \"y\": [223, 118, 63, 45, 7, 5, 2]}],\n",
       "                        {\"title\": {\"font\": {\"size\": 30}, \"text\": \"Accounts with more than 400 Tweets\"}, \"xaxis\": {\"automargin\": true, \"tickangle\": 45, \"tickfont\": {\"size\": 15}}, \"yaxis\": {\"title\": {\"font\": {\"size\": 20}, \"text\": \"Accounts\"}}},\n",
       "                        {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('26500950-cd08-4c79-950c-0cff87461b28');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "account_analysis = df.groupby(['author', 'account_category'])['content'].count().reset_index()\n",
    "account_analysis = Counter(account_analysis.account_category)\n",
    "\n",
    "account_type = list(account_analysis.keys())\n",
    "values = list(account_analysis.values())\n",
    "\n",
    "account_type = [x for _,x in sorted(zip(values ,account_type), reverse = True)]\n",
    "values = sorted(values, reverse = True)\n",
    "\n",
    "color_list = ['rgb(175, 88, 141)' if i == 'NonEnglish' else 'rgb(23,62,90)' for i in account_type]\n",
    "\n",
    "data = [go.Bar(\n",
    "            x=account_type,\n",
    "            y=values,\n",
    "            marker = dict(color = color_list),\n",
    "            opacity = .7\n",
    "    )]\n",
    "\n",
    "layout = go.Layout(title = dict(text = 'Accounts with more than 400 Tweets', font = dict(size = 30)), \n",
    "                   xaxis = dict(\n",
    "                       tickangle = 45, \n",
    "                       tickfont = dict(size = 15),\n",
    "                       automargin = True),\n",
    "                  yaxis = dict(title = dict(text = 'Accounts', font = dict(size = 20))))\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig, filename='basic-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, natural language processing can be computationally expensive. Since we are dealing with a large number of tweets, it makes sense to take a sample of the data that we want to work with. \n",
    "\n",
    "Here, I take a 30% sample of tweets from each account. \n",
    "\n",
    "Note: the 30% sample size is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annakot/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:5096: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.content = df.content.astype(str)\n",
    "\n",
    "# Create the groupby object of author and category. Note: every author is assigned one category\n",
    "df_sampled = df.groupby(['author', 'account_category'])['content']\n",
    "\n",
    "# Take a 30% sample from each group\n",
    "df_sampled = df_sampled.apply(lambda x: x.sample(frac=0.3, replace = False)).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be inclined to ask: why take a sample per account rather than just a sample of the overall dataset? When I first began expirimenting with this data, I tried to embed each individual tweet in a vector space. However, this procedure produced nonsensical results. It seems that there is not enough information contained in an individual tweet for us to gleen any useful unsights about the author, or discriminate between authors. \n",
    "\n",
    "However, if we were to combine tweets per author such that we have a single document for each author that is representative of everything that a particular author has ever tweeted, we will have more than enough information to give doc2vec a chance to work. \n",
    "\n",
    "There is just one problem: spaCy breaks if we try to parse long documents of text. As such, we will need to group our tweets by author after running the data through spaCy. \n",
    "\n",
    "Before we get to spaCy, we need to do some minor preprocessing. Here, I remove hyperlinks and and strip redundant whitepace from the text. I also expirimented with removing the RT symbol, but this did not seem to have any meaningful impact on our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'processing_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-58c5b1ee7ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# REMOVE ANY HYPERLINKS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprocessing_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlink_remover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf_sampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sampled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_remover\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'processing_functions'"
     ]
    }
   ],
   "source": [
    "# SAMPLE AND PRE-CLEANING\n",
    "\n",
    "# REMOVE RT SYMBOL\n",
    "#from processing_functions import rt_remover\n",
    "#df_sampled.content = df_sampled.content.apply(rt_remover)\n",
    "\n",
    "# REMOVE ANY HYPERLINKS\n",
    "from processing_functions import link_remover\n",
    "df_sampled.content = df_sampled.content.apply(link_remover)\n",
    "\n",
    "\n",
    "# STRIP ANY WHITESPACE ON EITHER SIDE OF THE TEXT\n",
    "df_sampled.content = df_sampled.content.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have done some basic pre-processing, we can begin to parse and clean our text with spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization, cleaning, and lemmatization with spaCy\n",
    "\n",
    "- Note: this takes approximately 5 - 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Token\n",
    "\n",
    "# This allows us to add custom attributes to tokens, in this case, hashtags and accounts\n",
    "Token.set_extension('is_hashtag', default = False, force = True)\n",
    "Token.set_extension('is_account', default = False, force = True)\n",
    "\n",
    "# These functions tell spaCy what should be considered a hashtage or account\n",
    "from processing_functions import hashtag_pipe\n",
    "from processing_functions import is_account_pipe\n",
    "\n",
    "# We can  disable pipeline objects to save time: disable = ['parser', 'etc']\n",
    "nlp = spacy.load(\"en\", disable = ['parser', 'ner'])\n",
    "\n",
    "# Here I add the two custom functions for hashtags and accounts to the pipeline\n",
    "nlp.add_pipe(hashtag_pipe)\n",
    "nlp.add_pipe(is_account_pipe)\n",
    "\n",
    "# And we're off!\n",
    "parsed_tweets = list(nlp.pipe(df_sampled.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "\n",
    "Before we train our gensim doc2vec model, we need to do some more cleaning. The clean_doc function is located in the processing_functions.py file in this repository. \n",
    "\n",
    "Note: While I tried running the following without lemmatizing or removing stop words, I did not find that this made any meaningful difference in terms of the clusters that do or do not form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processing_functions import clean_doc\n",
    "from processing_functions import clean_doc_no_lemma\n",
    "\n",
    "lemma = list(map(clean_doc, parsed_tweets))\n",
    "#no_lemma = list(map(clean_doc_no_lemma, parsed_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is perhaps the most critical to this analysis. We need to group the parsed and cleaned tweets by author, so that for each author, we have a 30% sample of everything that they have ever tweeted in a single list (as discussed above). We can easily do this by applying our own function to a Pandas groupby object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each row in parsed content is a list of lists\n",
    "df_sampled['parsed_content'] = lemma\n",
    "\n",
    "# lets flatten these so that each row only has one list\n",
    "from processing_functions import group_lists\n",
    "        \n",
    "df_grouped = df_sampled.groupby(['author', \n",
    "                    'account_category'])['parsed_content'].apply(group_lists).reset_index()\n",
    "\n",
    "# Convert the parsed content Series to a list for gensim\n",
    "parsed_content = list(df_grouped.parsed_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that out data is clean and in the proper format, we can go ahead and create our gensim doc2vec model. It never ceases to amaze me that we can implement such a poweful algorithm in just a few lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a doc2vec model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# parsed_content is the list of parsed text\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(parsed_content)]\n",
    "\n",
    "# Vector size of 300\n",
    "model = Doc2Vec(documents, vector_size=300, window=5, min_count=3, workers=6)\n",
    "\n",
    "arr_list = []\n",
    "\n",
    "for index in range(0, len(model.docvecs)):\n",
    "    arr_list.append(model.docvecs[index])\n",
    "    \n",
    "vec_array = np.stack(arr_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've cleaned, tokenized, and embeded our textual data in a vector space. Let's try to visualize our data and see where we are at. \n",
    "\n",
    "Since we are working with a 300 dimension dataset, we need to employ a dimension reduction technique if we want to visualize some of the patterns in our data. t-SNE is a great algorithm for this purpose, and we will employ this technique in the space below. \n",
    "\n",
    "By reducing the number of dimensions with t-SNE, we can visualize our data on a two-dimensional canvas. In the following chart the color of each individual datapoint will correspond to a label manually assigned by Professors Warren and Linville. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the t-SNE model.\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_35 = TSNE(n_components = 2, perplexity = 35, verbose = 0 , n_iter = 1000).fit_transform(vec_array)\n",
    "\n",
    "color_list = [\"#003f5c\", \"#ffa600\", \"#f95d6a\", \"#a05195\", \"#ff7c43\", \"#2f4b7c\", \"#665191\", \"#d45087\"]\n",
    "\n",
    "data = []\n",
    "for idx, i in  enumerate(df_grouped.account_category.unique()):\n",
    "    x = tsne_35[df_grouped.account_category == i, 0]\n",
    "    y = tsne_35[df_grouped.account_category == i, 1]\n",
    "    \n",
    "    data.append(go.Scatter(x = x, y = y, mode = 'markers', name = i, marker = dict(color = color_list[idx])))\n",
    "\n",
    "layout = go.Layout(title = dict(text = 't-SNE Scatter: Original Labels', font = dict(size = 30)), legend = dict(font = dict(size = 15)))\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Our accounts seperate into distinct clusters that are more or less in line with the labels assigned by Warren and Linville! This suggests that we will be able to use KMeans or GMM to cluster accounts in an unsupervised way. \n",
    "\n",
    "One particularly interesting result is that there appears to be two distinct groups of Right Trolls that Warren and Linville did not discriminate between. By analyzing the behavior of these groups and others, we may be able to gain additional insight into Russian tradecraft and the overall objective of the Russian Twitter campaign.\n",
    "\n",
    "Let's move on to our clustering algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kmeans\n",
    "\n",
    "One of the challenges of using KMeans or any clustering algorithms is how to choose an appropriate K. We can use a metric called inertia to help aid us in choosing how many clusters we should specify for our data. \n",
    "\n",
    "But what is inertia and how does it work? Inertia is essentially a measure of clustering quality. That is, good clustering has tight clusters, and samples in each cluster are bunched together. Inertia measures how spread out clusters are and uses the distance from each sample to the centroid of its cluster in its calculation. \n",
    "\n",
    "A good rule of thumb is to choose the \"elbow\" in the inertia plot, or the inflection point where inertia begins to decrease at a slower rate.\n",
    "\n",
    "Special shout-out to Hugo from Datacamp for teaching me about how inertia can be used to choose k, as well as laying the foundation for nearly everything that I know in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "inertias = []\n",
    "clusters = list(range(1,11))\n",
    "\n",
    "for k in clusters:\n",
    "    \n",
    "    model = KMeans(n_clusters = k)\n",
    "    \n",
    "    model.fit(vec_array)\n",
    "    \n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [go.Scatter(x = clusters, y = inertias, mode = 'lines', marker = dict(color = \"#003f5c\")),\n",
    "        go.Scatter(x = clusters, y = inertias, mode = 'markers', marker = dict(color = \"#2f4b7c\", size = 9))]\n",
    "\n",
    "layout = go.Layout(title = dict(text = 'Inertia Plot', font = dict(size = 30)), showlegend = False)\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inertia plot suggests that a k of five may be appropriate for our data. Let's go ahead and instantiate a KMeans model with k equal to five."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "kmeans_labels = kmeans.fit_predict(vec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\"#003f5c\", \"#ffa600\", \"#f95d6a\", \"#a05195\", \"#ff7c43\", \"#2f4b7c\", \"#665191\", \"#d45087\"]\n",
    "\n",
    "data = []\n",
    "for idx, i in  enumerate(np.unique(kmeans_labels)):\n",
    "    x = tsne_35[kmeans_labels == i, 0]\n",
    "    y = tsne_35[kmeans_labels == i, 1]\n",
    "    \n",
    "    data.append(go.Scatter(x = x, y = y, mode = 'markers', name = str(i), marker = dict(color = color_list[idx])))\n",
    "\n",
    "layout = go.Layout(title = dict(text = 't-SNE Scatter: KMeans Labels', font = dict(size = 30)), legend = dict(font = dict(size = 15)))\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks great! I am actually surprised at how well KMeans seems to be performing on our data. \n",
    "\n",
    "KMeans is a discriminative algorithm, and can be thought of as a hard clustering technique. That is, a single element can only be assigned to one cluster. This may be problematic when analyzing text, as we are likely to see overlap between clusters and would naturally expect more ambiguity in our cluster assignments. For example, the ideological bent of a particular twitter acount may be unclear, and it may be difficult to place into one category or another based on the text alone. \n",
    "\n",
    "To account for uncertainty in cluster assignments, we could turn to a soft clustering method where we account for uncertainty in class assignment. For example, Gaussian Mixture Models (GMMs) are generative algorithms that provide a probabilistic way of doing soft clustering. \n",
    "\n",
    "GMMs can loosly be thought of as an extension of KMeans, except that instead of placeing centroids in random locations in space, we place probability distributions. We then use the Expectation Maximiazation algorithm to discover parameters for each probability distribution for our K sources, and move the distributions around until convergence.\n",
    "\n",
    "GMMs allow us to calculate the probability that a sample belongs to a cluster. As such, each sample is assigned a probability that it belongs in each cluster. \n",
    "\n",
    "Let's go ahead and experiment with mixture models in the space below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models (Work in Progress)\n",
    "\n",
    "I will be using Gaussian Mixture Models in the space below, although it is possible to use different kinds of probability distirbutions in mixture models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "gmm = GMM(n_components=5).fit(vec_array)\n",
    "\n",
    "gmm_labels = gmm.predict(vec_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = [\"#003f5c\", \"#ffa600\", \"#f95d6a\", \"#a05195\", \"#ff7c43\", \"#2f4b7c\", \"#665191\", \"#d45087\"]\n",
    "\n",
    "data = []\n",
    "for idx, i in  enumerate(np.unique(gmm_labels)):\n",
    "    x = tsne_35[gmm_labels == i, 0]\n",
    "    y = tsne_35[gmm_labels == i, 1]\n",
    "    \n",
    "    data.append(go.Scatter(x = x, y = y, mode = 'markers', name = str(i), marker = dict(color = color_list[idx])))\n",
    "\n",
    "layout = go.Layout(title = '2D representation of doc2vec clusters using GMM')\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = np.arange(1, 20)\n",
    "models = [GMM(n, covariance_type='full', random_state=0).fit(vec_array)\n",
    "          for n in n_components]\n",
    "\n",
    "data = [go.Scatter(x = n_components, y = [m.bic(vec_array) for m in models], mode = 'lines', name = 'BIC'),\n",
    "       go.Scatter(x = n_components, y = [m.aic(vec_array) for m in models], mode = 'lines', name = 'AIC')]\n",
    "\n",
    "layout = go.Layout(xaxis = dict(title = 'n_components'))\n",
    "\n",
    "py.iplot(data)\n",
    "\n",
    "print('n_components associated with minimum AIC: ', \n",
    "      np.argmin([m.aic(vec_array) for m in models]) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.predict_proba(vec_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird. I'm getting some results that I didn't expect, particularly how that class labels looks exactly the same as those from KMeans, and how the AIC/BIC chart looks. \n",
    "\n",
    "After thinking it over, I think that we may be falling victim to the curse of dimensionality. Since I am working with a relatively short and wide dataset, we might expect individual datapoints or even clusters of datapoints to be far apart in terms of distance. If this is the case, then the porbabilities that our model generates will be extremly low or extremly high, and this appears to be the case.\n",
    "\n",
    "At any rate, I will be sticking with KMeans as I move forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Now that we have completed our KMeans cluster assignments, it's time to see how they stack up against  Linville and Warrens labeling scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped['kmeans_labels'] = kmeans_labels\n",
    "\n",
    "df_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "\n",
    "for row in df_grouped.iterrows():\n",
    "    \n",
    "    try: \n",
    "        label_dict[row[1].account_category].append(row[1].kmeans_labels)\n",
    "    \n",
    "    except:\n",
    "        label_dict[row[1].account_category] = [row[1].kmeans_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to compute the conditional probability of a cluster label given the label assigned by Linville and Warren. This is trickier than it seems, and I could not think of a better way to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [0, 1, 2, 3, 4]\n",
    "\n",
    "prob_df = pd.DataFrame()\n",
    "\n",
    "for i in label_dict.keys():\n",
    "    count = dict(Counter(label_dict[i]))\n",
    "    length = sum(list(count.values()))\n",
    "\n",
    "    prob_dict = {}\n",
    "\n",
    "    for key in choices:\n",
    "        try:\n",
    "            prob_dict[key]= count[key]/length\n",
    "        \n",
    "        except:\n",
    "            prob_dict[key]= 0\n",
    "    \n",
    "    row = pd.DataFrame(pd.Series(prob_dict)).T\n",
    "    \n",
    "    row.index = [i]\n",
    "    \n",
    "    prob_df = pd.concat([prob_df, row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a lot of work!  We now have a dataframe of conditional probabilities. That is given a label by Warren and Linville, we have a conditional probability for each KMeans cluster label.  This is more clear if we view the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table above indicates that for the most part, our clusters align pretty well with Warren and Linville's labels. Let's go ahead and visualize this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df_t = prob_df.T\n",
    "\n",
    "colors = [\"#003f5c\", \"#58508d\", \"#bc5090\", \"#ff6361\", \"#ffa600\"]\n",
    "data = []\n",
    "for idx, label in enumerate(list(prob_df_t.columns)[:6]):  \n",
    "    data.append(go.Bar(x = list(prob_df_t.index), y = list(prob_df_t[label]), \n",
    "                       marker =  dict(color = colors), opacity = 0.9))\n",
    "    \n",
    "fig = tools.make_subplots(rows=2, cols=3, subplot_titles=('LeftTroll', 'HashtagGamer','RightTroll', \n",
    "                                                          'NewsFeed', 'NonEnglish', 'Commercial'))\n",
    "fig.append_trace(data[0], 1, 1)\n",
    "fig.append_trace(data[1], 1, 2)\n",
    "fig.append_trace(data[2], 1, 3)\n",
    "fig.append_trace(data[3], 2, 1)\n",
    "fig.append_trace(data[4], 2, 2)\n",
    "fig.append_trace(data[5], 2, 3)\n",
    "\n",
    "fig['layout'].update(height=700, \n",
    "                     title= dict(text = 'Conditional Probability of KMeans Assignment Given a Label', \n",
    "                                 font = dict(size = 27)),\n",
    "                     showlegend = False)\n",
    "\n",
    "for i in range(1, 7):\n",
    "    fig['layout']['yaxis' + str(i)].update(range = [0, 1], nticks = 10)\n",
    "\n",
    "\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to check our class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_grouped.account_category.copy()\n",
    "x = np.copy(vec_array)\n",
    "\n",
    "print(Counter(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the Limited number of samples, I am going to classify NonEnglish, Commercial, and Unknown as Other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_list = ['NonEnglish','Commercial', 'Unknown']\n",
    "\n",
    "y = y.apply(lambda x: 'Other' if x in other_list else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y)\n",
    "\n",
    "sm = SMOTE(random_state = 123, k_neighbors = 6)\n",
    "\n",
    "x_res, y_res = sm.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "logreg = OneVsRestClassifier(LogisticRegression(solver = 'liblinear'))\n",
    "\n",
    "logreg.fit(x_res, y_res)\n",
    "\n",
    "y_pred = logreg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Score: \",accuracy_score(y_test, y_pred), \"\\n\", \"\\n\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = OneVsRestClassifier(SVC(gamma = 'auto'))\n",
    "\n",
    "param_grid = {'estimator__C': [1, 3, 5, 7]}\n",
    "\n",
    "grid_svc = GridSearchCV(estimator = svc, param_grid = param_grid, \n",
    "                        scoring = 'accuracy', n_jobs = -1, verbose = 0,\n",
    "                       cv = 4)\n",
    "\n",
    "grid_svc.fit(x_res, y_res)\n",
    "\n",
    "y_pred_svc = grid_svc.predict(x_test)\n",
    "\n",
    "print('SVM Accuracy Score: ',accuracy_score(y_test, y_pred_svc), \"\\n\", \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred_svc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 123, n_estimators = 10)\n",
    "\n",
    "\n",
    "params_rf = {'max_depth':[2,3,4,5,6],\n",
    "             'min_samples_leaf': [0.04, 0.06, 0.08],\n",
    "             'max_features': [0.2, 0.4, 0.6, 0.8],\n",
    "             'criterion':['gini', 'entropy']}\n",
    "             \n",
    "grid_rf = GridSearchCV(estimator = rf, param_grid = params_rf, \n",
    "                       scoring = 'accuracy', cv= 10, n_jobs = -1, iid = True)\n",
    "\n",
    "\n",
    "grid_rf.fit(x_res, y_res)\n",
    "\n",
    "y_pred_rf = grid_rf.predict(x_test)\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred_rf), \"\\n\", \"\\n\")\n",
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid = {\n",
    "  'learning_rate': np.arange(0.05, 1.05, .10),\n",
    "  'n_estimators' : [50],\n",
    "  'subsample' : np.arange(0.05, 1.05, .05),\n",
    "  'max_depth': [2,4,6]\n",
    "  }\n",
    "\n",
    "xg_cl = xgb.XGBClassifier(objective = 'multi:softmax')\n",
    "\n",
    "randomized_xg_cl = RandomizedSearchCV(estimator = xg_cl, \n",
    "                                      param_distributions = param_grid,\n",
    "                                      n_iter = 5, \n",
    "                                      cv = 5,\n",
    "                                      scoring = 'accuracy',\n",
    "                                      n_jobs = -1,\n",
    "                                      verbose = 0)\n",
    "\n",
    "randomized_xg_cl.fit(x_res, y_res)\n",
    "\n",
    "print(\"Best Accuracy Score Train CV: \", randomized_xg_cl.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = randomized_xg_cl.predict(x_test)\n",
    "\n",
    "print('XGBoost Test Accuracy: ', accuracy_score(y_test, y_pred), \"\\n\", \"\\n\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
